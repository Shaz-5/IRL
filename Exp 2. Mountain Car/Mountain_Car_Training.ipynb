{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7UfNYU07lAJ"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random, copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EYg_4xDpLnr",
    "outputId": "0a279861-5efd-4b51-a251-6b07532fc42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Episode Steps: 200\n",
      "\n",
      "Discretization Details:\n",
      "\n",
      "Position: -1.2 to 0.6, Bin Size: 0.014999999999999998\n",
      "Velocity: -0.07 to 0.07, Bin Size: 0.0011666666666666668\n"
     ]
    }
   ],
   "source": [
    "# function to take a continuous state 's' from the environment and discretize it into a grid position\n",
    "def get_state(s):\n",
    "    xpos = min(discretization - 1, round((s[0] - xmin) / x_binsize))    # s[0] and s[1] - position and velocity of the agent\n",
    "    vpos = min(discretization - 1, round((s[1] - vmin) / v_binsize))\n",
    "    return int(xpos), int(vpos)     # indices in the discretized state space\n",
    "\n",
    "# create the Mountain Car environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env.seed(42)\n",
    "\n",
    "# hyperparameters\n",
    "alpha = 0.1     # learning rate\n",
    "gamma = 0.99    # discount rate\n",
    "epsilon = 0.05  # exploration rate\n",
    "\n",
    "# initialize Q-table with zeros\n",
    "Q = [[[0 for _ in range(3)] for _ in range(120)] for _ in range(120)]\n",
    "Q_optim = copy.deepcopy(Q)    # copy of Q to store the optimized Q-values later\n",
    "\n",
    "# Minimum steps needed (initialized to maximum possible steps)\n",
    "min_steps = env._max_episode_steps + 1\n",
    "\n",
    "# environment information\n",
    "e = env.env\n",
    "discretization = 120    # number of bins\n",
    "\n",
    "# discretize state space\n",
    "xmin = e.min_position                         # min value for position\n",
    "xmax = e.max_position                         # max value for position\n",
    "x_binsize = (xmax - xmin) / discretization    # size of each bin in the discretized space\n",
    "\n",
    "vmin = -1 * e.max_speed                       # min value for velocity\n",
    "vmax = e.max_speed                            # max value for velocity\n",
    "v_binsize = (vmax - vmin) / discretization\n",
    "\n",
    "\n",
    "print(\"Maximum Episode Steps:\", env._max_episode_steps)\n",
    "print(\"\\nDiscretization Details:\\n\")\n",
    "print(f\"Position: {xmin} to {xmax}, Bin Size: {x_binsize}\")\n",
    "print(f\"Velocity: {vmin} to {vmax}, Bin Size: {v_binsize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWF1Vv_btACe",
    "outputId": "e026497c-6aea-424f-bd2a-393a831391c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000 -> Steps: 200\n",
      "Episode: 4000 -> Steps: 200\n",
      "Episode: 6000 -> Steps: 200\n",
      "Episode: 8000 -> Steps: 200\n",
      "Episode: 10000 -> Steps: 200\n",
      "Episode: 12000 -> Steps: 200\n",
      "Episode: 14000 -> Steps: 200\n",
      "Episode: 16000 -> Steps: 200\n",
      "Episode: 18000 -> Steps: 200\n",
      "Episode: 20000 -> Steps: 200\n"
     ]
    }
   ],
   "source": [
    "time = 0\n",
    "\n",
    "while time < 20000:   # or min_steps > 260:\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    steps = 0\n",
    "\n",
    "    greed_pol = random.randint(0, 9)\n",
    "\n",
    "    while True:     # episode\n",
    "        steps += 1\n",
    "        # if time % 2000 == 0:\n",
    "        #     env.render()\n",
    "\n",
    "        x, v = get_state(obs)\n",
    "        a = random.randint(0, 2)\n",
    "\n",
    "        # xxploration or exploitation based on epsilon-greedy strategy\n",
    "        if greed_pol == 0 or random.random() < 1 - epsilon:\n",
    "            a = np.argmax(np.array(Q[x][v]))\n",
    "\n",
    "        obs, R, done, info = env.step(a)\n",
    "\n",
    "        x1, v1 = get_state(obs)\n",
    "\n",
    "        score += R\n",
    "\n",
    "        Q[x][v][a] += alpha * (R + gamma * max(Q[x1][v1]) - Q[x][v][a])   # Q(s, a) ← Q(s, a) + α ( R + γ * max{a'} * Q(s', a') - Q(s, a)) Bellman Eq\n",
    "\n",
    "        if done:    # episode terminated\n",
    "            break\n",
    "\n",
    "    time += 1\n",
    "    if time%2000 == 0:\n",
    "        print(f\"Episode: {time} -> Steps: {steps}\")\n",
    "\n",
    "    # update the lowest steps and Q values\n",
    "    if greed_pol == 0 and steps < min_steps:\n",
    "        min_steps = steps\n",
    "        Q_optim = copy.deepcopy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoB-_QfW2vEw",
    "outputId": "ead933fd-1e21-405d-ef07-f7f462eea341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Steps: 159\n"
     ]
    }
   ],
   "source": [
    "print(f\"Min Steps: {min_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yuR8JiB36Ge"
   },
   "outputs": [],
   "source": [
    "# Saving Q_optim\n",
    "\n",
    "file_path = 'Q_opt.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(Q, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iy32zdZky2XN"
   },
   "outputs": [],
   "source": [
    "# evaluate the trained Q-learning agent's performance in the environment\n",
    "\n",
    "while input('Continue?: ').lower() == \"y\":\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        x, v = get_state(obs)\n",
    "        a = np.argmax(np.array(Q[x][v]))\n",
    "        obs, R, done, info = env.step(a)\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXDfXzhcLgVR"
   },
   "outputs": [],
   "source": [
    "# function for calculating the probability density function (PDF) of a normal distribution\n",
    "\n",
    "from math import exp, sqrt, pi\n",
    "\n",
    "def calc_pdf(a, m, s):\n",
    "    a = (a - m) / s\n",
    "    return (exp(-(a) ** 2 / (2)) / (sqrt(2 * pi))) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yErGN8BNP09L"
   },
   "outputs": [],
   "source": [
    "def get_value_function(Q,mean,scale,i,alpha=0.1,gamma=0.99,epsilon=0.05):\n",
    "\n",
    "    V=[[0 for v in range(120)] for x in range(120)]\n",
    "\n",
    "    time=0\n",
    "    while time<10000:     # or min_steps>260:\n",
    "        obs = env.reset()\n",
    "\n",
    "        while True:\n",
    "            x,v = get_state(obs)\n",
    "            #a=random.randint(0,2)\n",
    "            #if random.random()<1-epsilon:\n",
    "              #print(Q[x][v])\n",
    "            a=np.argmax(np.array(Q[x][v]))      # choose an action using the policy represented by Q-values\n",
    "\n",
    "            obs,R,done,info=env.step(a)\n",
    "            R = calc_pdf(x,mean,scale)           # calculate a reward using a probability density function (PDF)\n",
    "\n",
    "            x1,v1 = get_state(obs)          # get next state (x1, v1)\n",
    "\n",
    "            V[x][v] += alpha*(R + gamma*V[x1][v1] - V[x][v])    # update value function using the Q-learning update rule\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        time+=1\n",
    "        if time%1000 == 0:\n",
    "            print(f\"Episode: {time}\")\n",
    "\n",
    "    print(V)\n",
    "\n",
    "    with open('V'+str(i), 'wb') as file:\n",
    "        pickle.dump(V, file)\n",
    "\n",
    "    return V"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
