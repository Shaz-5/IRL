{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qmL-nPOiFxSP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actions:\n",
        "* 0 - Left\n",
        "* 1 - Down\n",
        "* 2 - Right\n",
        "* 3 - Up\n",
        "\n",
        "Agent Position:  1.1"
      ],
      "metadata": {
        "id": "g_Cqot_jGN1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining GridWorld Environment Class\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class MyGridWorld:\n",
        "\n",
        "    size = 5                        # 5x5 grid\n",
        "    RewardGrid = np.zeros([5, 5])   # grid representing rewards\n",
        "    RewardGrid[0][4] = 1            # sets reward in the top-right cell to 1\n",
        "    PositionGrid = np.zeros([5, 5]) # grid representing the current position of the agent\n",
        "    PositionGrid[4][0] = 1.1        # sets agent's initial position in the bottom-left cell\n",
        "    action_space = 4                # no. of possible actions\n",
        "    noisyMoveChance = 0.3           # probability of noisy move\n",
        "    currI = 4                       # row index\n",
        "    currJ = 0                       # col index\n",
        "    DoneStatus = False              # whether the episode is terminated\n",
        "    EnableNoise = True              # enable or disable noise\n",
        "    observation_spaces = size * size # total no. of observations\n",
        "\n",
        "    # initialize the environment with default values\n",
        "    def __init__(self, size=5, noisyMoveChance=0.3, EnableNoise=True):\n",
        "        self.basic_reset()\n",
        "        self.EnableNoise = EnableNoise\n",
        "        if 0 < size:\n",
        "            self.size = int(size)\n",
        "            self.RewardGrid = np.zeros([size, size])\n",
        "            self.RewardGrid[0][size-1] = 1\n",
        "            self.PositionGrid = np.zeros([size, size])\n",
        "            self.PositionGrid[size-1][0] = 1.1\n",
        "            self.observation_spaces = self.size * self.size\n",
        "            self.currI = size-1\n",
        "            self.currJ = 0\n",
        "            self.observation_spaces = self.size * self.size\n",
        "        if 0 < noisyMoveChance < 1:     # probability value\n",
        "            self.noisyMoveChance = noisyMoveChance\n",
        "\n",
        "    # resets the environment to its initial state\n",
        "    def basic_reset(self):\n",
        "        self.size = 5\n",
        "        self.RewardGrid = np.zeros([5, 5])\n",
        "        self.RewardGrid[0][4] = 1\n",
        "        self.PositionGrid = np.zeros([5, 5])\n",
        "        self.PositionGrid[4][0] = 1.1\n",
        "        self.action_space = 4\n",
        "        self.noisyMoveChance = 0.3\n",
        "        self.currI = 4\n",
        "        self.currJ = 0\n",
        "        self.DoneStatus = False\n",
        "        self.EnableNoise = True\n",
        "        self.observation_spaces = self.size * self.size\n",
        "\n",
        "    # reset environment with parameters\n",
        "    def reset(self, size=5, noisyMoveChance=0.3, EnableNoise=True):\n",
        "        self.__init__(size, noisyMoveChance, EnableNoise)\n",
        "        return self.currI * self.size + self.currJ         # current state of the agent\n",
        "\n",
        "    # print the reward grid\n",
        "    def print_reward_grid(self):\n",
        "        for i in range(len(self.RewardGrid)):\n",
        "            for j in range(len(self.RewardGrid[0])):\n",
        "                print(self.RewardGrid[i][j], end=' ')\n",
        "            print()\n",
        "\n",
        "    # print the position grid\n",
        "    def print_position_grid(self):\n",
        "        for i in range(len(self.PositionGrid)):\n",
        "            for j in range(len(self.PositionGrid[0])):\n",
        "                print(self.PositionGrid[i][j], end=' ')\n",
        "\n",
        "    # print the current state of the position grid\n",
        "    def render(self):\n",
        "        self.print_position_grid()\n",
        "\n",
        "    # return position grid\n",
        "    def get_position_grid(self):\n",
        "        return self.PositionGrid\n",
        "\n",
        "    # return no. of actions\n",
        "    def get_available_moves(self):\n",
        "        return self.action_space\n",
        "\n",
        "    # return size of grid\n",
        "    def get_size(self):\n",
        "        return self.size\n",
        "\n",
        "    # takes an action and updates the agent's position\n",
        "    def move(self, action):\n",
        "        rand_num = random.random()\n",
        "        if self.EnableNoise and rand_num <= self.noisyMoveChance:\n",
        "            self.make_noisy_move(action)\n",
        "        else:\n",
        "            self.make_proper_move(action)\n",
        "        return self.currI, self.currJ, self.currI * self.size + self.currJ, self.RewardGrid[self.currI][self.currJ], self.DoneStatus\n",
        "\n",
        "    # noisy move with random action\n",
        "    def make_noisy_move(self, action):\n",
        "        rand_num = random.randint(0, 3)\n",
        "        self.make_proper_move(rand_num)\n",
        "\n",
        "    # proper move based on given action\n",
        "    def make_proper_move(self, action):\n",
        "        if action == 0:  # Left\n",
        "            if 0 < self.currJ:\n",
        "                self.PositionGrid[self.currI][self.currJ] = 0\n",
        "                self.currJ -= 1\n",
        "                self.PositionGrid[self.currI][self.currJ] = 1.1\n",
        "\n",
        "        elif action == 1:  # Down\n",
        "            if self.currI < self.size - 1:\n",
        "                self.PositionGrid[self.currI][self.currJ] = 0\n",
        "                self.currI += 1\n",
        "                self.PositionGrid[self.currI][self.currJ] = 1.1\n",
        "\n",
        "        elif action == 2:  # Right\n",
        "            if self.currJ < self.size - 1:\n",
        "                self.PositionGrid[self.currI][self.currJ] = 0\n",
        "                self.currJ += 1\n",
        "                self.PositionGrid[self.currI][self.currJ] = 1.1\n",
        "\n",
        "        elif action == 3:  # Up\n",
        "            if 0 < self.currI:\n",
        "                self.PositionGrid[self.currI][self.currJ] = 0\n",
        "                self.currI -= 1\n",
        "                self.PositionGrid[self.currI][self.currJ] = 1.1\n",
        "\n",
        "        if self.currI == 0 and self.currJ == self.size - 1:   # termination condition reached\n",
        "            self.DoneStatus = True\n",
        "\n",
        "    # call move method on action and return output of it\n",
        "    def step(self, action):\n",
        "        return self.move(action)"
      ],
      "metadata": {
        "id": "lj8yhWwRF7L1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-Learning model training class\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class MyGridWorldTrainer:\n",
        "\n",
        "    env = []                # an instance of environment\n",
        "    Q = []                  # q value matrix\n",
        "    matrix = []             # matrix with actions corresponding to the highest q values for each state\n",
        "    Trajectories = []       # list of trajectories obtained during training\n",
        "    DirectionalMatrix = []  # matrix with arrows based on the highest q values\n",
        "\n",
        "    # train a q learning model\n",
        "    def train_model(self, model):\n",
        "        env = self.env\n",
        "        alpha = 0.6    # learning rate\n",
        "        gamma = 0.9    # discount factor\n",
        "        Q = np.zeros([env.observation_spaces, env.action_space])\n",
        "\n",
        "        for episode in range(1, 10001):\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            state = env.reset()    # reset env\n",
        "\n",
        "            while not done:\n",
        "                if episode < 500:    # epsilon-greedy strategy\n",
        "                    action = random.randint(0, 3)\n",
        "                else:\n",
        "                    action = np.argmax(Q[state])\n",
        "                i, j, state2, reward, done = env.step(action)     # takes an action\n",
        "                Q[state, action] += alpha * (reward + gamma * np.max(Q[state2]) - Q[state, action])    # update q value\n",
        "                total_reward += reward\n",
        "                state = state2\n",
        "\n",
        "        self.Q = Q    # learned q values matrix\n",
        "        return Q\n",
        "\n",
        "    # get optimal directions from learned q values\n",
        "    def get_directions(self, Q):\n",
        "        matrix = []\n",
        "\n",
        "        for i in range(0, 25):\n",
        "            matrix.append(np.argmax(Q[i]))      # appends the index of the action with maximum Q-value\n",
        "        matrix = np.reshape(matrix, (5, 5))\n",
        "\n",
        "        DirectionalMatrix = []\n",
        "        for i in range(5):\n",
        "            row = []\n",
        "            for j in range(5):\n",
        "                if matrix[i][j] == 0:\n",
        "                    row.append('\\u2190')    # left symbol\n",
        "                elif matrix[i][j] == 1:\n",
        "                    row.append('\\u2193')    # down symbol\n",
        "                elif matrix[i][j] == 2:\n",
        "                    row.append('\\u2192')    # right symbol\n",
        "                elif matrix[i][j] == 3:\n",
        "                    row.append('\\u2191')    # up symbol\n",
        "            DirectionalMatrix.append(row)\n",
        "\n",
        "        self.DirectionalMatrix = DirectionalMatrix\n",
        "        self.matrix = matrix\n",
        "        return matrix\n",
        "\n",
        "    # generate trajectories based on optimal actions\n",
        "    def get_trajectories(self, matrix, num_trajectories):\n",
        "        Trajectories = []\n",
        "\n",
        "        for iters in range(num_trajectories):\n",
        "            path = []       # list for a single trajectory\n",
        "            done = False\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            path.append(state)\n",
        "            i = int(state / self.env.size)    # row index\n",
        "            j = state % self.env.size         # col index\n",
        "\n",
        "            # trajectory loop\n",
        "            while not done:\n",
        "                action = matrix[i][j]       # retrieve action\n",
        "                i, j, state2, reward, done = self.env.step(action)    # take action\n",
        "                total_reward += reward\n",
        "                state = state2          # update state\n",
        "                path.append(state)\n",
        "\n",
        "            Trajectories.append(path)\n",
        "\n",
        "        self.Trajectories = Trajectories\n",
        "        return Trajectories\n",
        "\n",
        "    # all training functions\n",
        "    def all_in_one(self, model, num_trajectories):\n",
        "        self.env = model\n",
        "        Q = self.train_model(model)\n",
        "        matrix = self.get_directions(Q)\n",
        "        return self.get_trajectories(matrix, num_trajectories)"
      ],
      "metadata": {
        "id": "cKSOh0qm9GyK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample case\n",
        "sample_grid = MyGridWorld()\n",
        "sample_grid_trainer = MyGridWorldTrainer()\n",
        "sample_trajectories = sample_grid_trainer.all_in_one(sample_grid, 20)     # training (Q Learning)\n",
        "\n",
        "print('Policy: \\n')\n",
        "for direction in sample_grid_trainer.matrix:\n",
        "    print(direction)\n",
        "\n",
        "print('\\nPolicy (directions): \\n')\n",
        "for row in sample_grid_trainer.DirectionalMatrix:\n",
        "    print(row)\n",
        "\n",
        "print('\\nQ value matrix: \\n')\n",
        "for row in sample_grid_trainer.Q:   # Q value for each state action pair\n",
        "    print(row)\n",
        "\n",
        "# print('\\nTrajectories: \\n')\n",
        "# for trajectory in sample_trajectories:\n",
        "#     print(trajectory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyMpFMqjNbDu",
        "outputId": "b2369f92-df64-4ebf-e3d0-e8a241a67e71"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy: \n",
            "\n",
            "[2 2 2 2 0]\n",
            "[2 3 3 2 3]\n",
            "[3 3 2 2 3]\n",
            "[3 2 2 3 3]\n",
            "[2 2 2 3 3]\n",
            "\n",
            "Policy (directions): \n",
            "\n",
            "['→', '→', '→', '→', '←']\n",
            "['→', '↑', '↑', '→', '↑']\n",
            "['↑', '↑', '→', '→', '↑']\n",
            "['↑', '→', '→', '↑', '↑']\n",
            "['→', '→', '→', '↑', '↑']\n",
            "\n",
            "Q value matrix: \n",
            "\n",
            "[0.35623152 0.37037826 0.54984064 0.37140916]\n",
            "[0.45128763 0.45148873 0.70380835 0.44746223]\n",
            "[0.50542982 0.51972487 0.81854707 0.50971567]\n",
            "[0.61898363 0.64479207 0.97447639 0.62349709]\n",
            "[0. 0. 0. 0.]\n",
            "[0.36355492 0.37285528 0.58706431 0.33686081]\n",
            "[0.39064186 0.36731456 0.39235869 0.64375304]\n",
            "[0.47802167 0.46393606 0.4872644  0.64325159]\n",
            "[0.53122735 0.51640654 0.80016132 0.50318315]\n",
            "[0.56880259 0.55424806 0.5813022  0.99930327]\n",
            "[0.32161518 0.32540554 0.32598789 0.48933005]\n",
            "[0.35166137 0.33654011 0.3485128  0.53743295]\n",
            "[0.37520391 0.37757782 0.50609234 0.39336873]\n",
            "[0.40962922 0.41948025 0.76108292 0.41814939]\n",
            "[0.47413317 0.45295068 0.47585676 0.89411551]\n",
            "[0.28748166 0.27619602 0.2875726  0.40346224]\n",
            "[0.30697137 0.29237871 0.45072779 0.29538937]\n",
            "[0.33012947 0.33059948 0.45532036 0.33191592]\n",
            "[0.37731171 0.35716401 0.38572342 0.55459629]\n",
            "[0.39458752 0.37984519 0.38065822 0.74349706]\n",
            "[0.24640747 0.24739412 0.3690375  0.24466168]\n",
            "[0.25999525 0.26816897 0.39498803 0.27116937]\n",
            "[0.31427858 0.3040969  0.41647706 0.31992172]\n",
            "[0.31756066 0.31801594 0.3226708  0.50935528]\n",
            "[0.3410766  0.33931122 0.34789906 0.55524464]\n"
          ]
        }
      ]
    }
  ]
}