{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qmL-nPOiFxSP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actions:\n",
        "* 0 - Left\n",
        "* 1 - Down\n",
        "* 2 - Right\n",
        "* 3 - Up\n",
        "\n",
        "Agent Position:  1.1"
      ],
      "metadata": {
        "id": "g_Cqot_jGN1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining GridWorld Environment Class\n",
        "\n",
        "class myGridWorld:\n",
        "\n",
        "    size=5                        # 5x5 grid\n",
        "    RewardGrid=np.zeros([5,5])    # grid representing rewards\n",
        "    RewardGrid[0][4]=1            # sets reward in the top-right cell to 1\n",
        "    PositionGrid=np.zeros([5,5])  # grid representing current position of agent\n",
        "    PositionGrid[4][0]=1.1        # sets agent's initial position in the bottom-left cell\n",
        "    action_space=4                # no. of possible actions\n",
        "    noisyMoveChance=0.3           # probability of noisy move\n",
        "    currI=4                       # row index\n",
        "    currJ=0                       # col index\n",
        "    DoneStatus=False              # whether the episode is terminated\n",
        "    EnableNoise=True              # enable or disable noise\n",
        "    observation_spaces=size*size  # total no. of observations\n",
        "\n",
        "    # initialize the environment with default values\n",
        "    def __init__(self,size=5,noisyMoveChance=0.3,EnableNoise=True):\n",
        "        self.basicReset()\n",
        "        self.EnableNoise=EnableNoise\n",
        "        if(0<size):\n",
        "            self.size=int(size)\n",
        "            self.RewardGrid=np.zeros([size,size])\n",
        "            self.RewardGrid[0][size-1]=1\n",
        "            self.PositionGrid=np.zeros([size,size])\n",
        "            self.PositionGrid[size-1][0]=1.1\n",
        "            self.observation_spaces=self.size*self.size\n",
        "            self.currI=size-1\n",
        "            self.currJ=0\n",
        "            self.observation_spaces=self.size*self.size\n",
        "        if(0<noisyMoveChance and noisyMoveChance<1):     # probability value\n",
        "            self.noisyMoveChance=noisyMoveChance\n",
        "\n",
        "    # resets the environment to its initial state\n",
        "    def basicReset(self):\n",
        "          self.size=5\n",
        "          self.RewardGrid=np.zeros([5,5])\n",
        "          self.RewardGrid[0][4]=1\n",
        "          self.PositionGrid=np.zeros([5,5])\n",
        "          self.PositionGrid[4][0]=1.1\n",
        "          self.action_space=4\n",
        "          self.noisyMoveChance=0.3\n",
        "          self.currI=4\n",
        "          self.currJ=0\n",
        "          self.DoneStatus=False\n",
        "          self.EnableNoise=True\n",
        "          self.observation_spaces=self.size*self.size\n",
        "\n",
        "    # reset environment with parameters\n",
        "    def reset(self,size=5,noisyMoveChance=0.3,EnableNoise=True):\n",
        "        self.__init__(size,noisyMoveChance,EnableNoise)\n",
        "        return self.currI*self.size+self.currJ         #current state of agent\n",
        "\n",
        "    # print the reward grid\n",
        "    def printRewardGrid(self):\n",
        "        for i in range(len(self.RewardGrid)):\n",
        "            for j in range(len(self.RewardGrid[0])):\n",
        "                print(self.RewardGrid[i][j],end=' ')\n",
        "            print()\n",
        "\n",
        "    # print the position grid\n",
        "    def printPositionGrid(self):\n",
        "        for i in range(len(self.PositionGrid)):\n",
        "            for j in range(len(self.PositionGrid[0])):\n",
        "                print(self.PositionGrid[i][j],end=' ')\n",
        "\n",
        "    # print the current state of the position grid\n",
        "    def render(self):\n",
        "        self.printPositionGrid()\n",
        "\n",
        "    # return position grid\n",
        "    def getPositionGrid(self):\n",
        "        return self.PositionGrid\n",
        "\n",
        "    # return no. of actions\n",
        "    def getAvailableMoves(self):\n",
        "        return self.action_space\n",
        "\n",
        "    # return size of grid\n",
        "    def getSize(self):\n",
        "        return self.size\n",
        "\n",
        "    # takes an action and updates the agent's position\n",
        "    def move(self,action):\n",
        "        randNum=random.random()\n",
        "        if(self.EnableNoise and randNum<=self.noisyMoveChance):\n",
        "            self.makeNoisyMove(action)\n",
        "        else:\n",
        "            self.makeProperMove(action)\n",
        "        return self.currI,self.currJ,self.currI*self.size+self.currJ,self.RewardGrid[self.currI][self.currJ],self.DoneStatus\n",
        "\n",
        "    # noisy move with random action\n",
        "    def makeNoisyMove(self,action):\n",
        "        randNum=random.randint(0,3)\n",
        "        self.makeProperMove(randNum)\n",
        "\n",
        "    # proper move based on given action\n",
        "    def makeProperMove(self,action):\n",
        "        if(action==0):  # Left\n",
        "            if(0<self.currJ):\n",
        "                self.PositionGrid[self.currI][self.currJ]=0\n",
        "                self.currJ-=1\n",
        "                self.PositionGrid[self.currI][self.currJ]=1.1\n",
        "\n",
        "        elif(action==1):  # Down\n",
        "            if(self.currI<self.size-1):\n",
        "                self.PositionGrid[self.currI][self.currJ]=0\n",
        "                self.currI+=1\n",
        "                self.PositionGrid[self.currI][self.currJ]=1.1\n",
        "\n",
        "        elif(action==2):  # Right\n",
        "            if(self.currJ<self.size-1):\n",
        "                self.PositionGrid[self.currI][self.currJ]=0\n",
        "                self.currJ+=1\n",
        "                self.PositionGrid[self.currI][self.currJ]=1.1\n",
        "\n",
        "        elif(action==3):  # Up\n",
        "            if(0<self.currI):\n",
        "                self.PositionGrid[self.currI][self.currJ]=0\n",
        "                self.currI-=1\n",
        "                self.PositionGrid[self.currI][self.currJ]=1.1\n",
        "\n",
        "        if(self.currI==0 and self.currJ==self.size-1):   # termination condition reached\n",
        "            self.DoneStatus=True\n",
        "\n",
        "    # call move method on action and return output of it\n",
        "    def step(self,action):\n",
        "        return self.move(action)"
      ],
      "metadata": {
        "id": "lj8yhWwRF7L1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-Learning model training class\n",
        "\n",
        "class myGridWorldTrainer:\n",
        "\n",
        "    env=[]                # an instance of environment\n",
        "    Q=[]                  # q value matrix\n",
        "    matrix=[]             # matrix with actions corresponding to the highest q values for each state\n",
        "    Trajectories=[]       # list of trajectories obtained during training\n",
        "    DirectionalMatrix=[]  # matrix with arrows based on the highest q values\n",
        "\n",
        "    # train a q learning model\n",
        "    def trainModel(self,model):\n",
        "        env=self.env\n",
        "        alpha = 0.6    # learning rate\n",
        "        gamma = 0.9    # discount factor\n",
        "        Q = np.zeros([env.observation_spaces, env.action_space])\n",
        "\n",
        "        for episode in range(1,10001):\n",
        "            done = False\n",
        "            TotalReward = 0\n",
        "            state = env.reset()    # reset env\n",
        "\n",
        "            while done != True:\n",
        "                if(episode<500):    # epsilon-greedy strategy\n",
        "                    action = random.randint(0,3)\n",
        "                else:\n",
        "                    action=np.argmax(Q[state])\n",
        "                    i,j,state2, reward, done = env.step(action)     # takes an action\n",
        "                    Q[state,action] += alpha * (reward + gamma* np.max(Q[state2]) - Q[state,action])    #update q value\n",
        "                    TotalReward += reward\n",
        "                    state = state2\n",
        "\n",
        "        self.Q=Q    # learned q values matrix\n",
        "        return Q\n",
        "\n",
        "    # get optimal directions from learned q values\n",
        "    def getDirections(self,Q):\n",
        "        matrix=[]\n",
        "\n",
        "        for i in range(0,25):\n",
        "            matrix.append(np.argmax(Q[i]))      # appends the index of the action with maximum Q-value\n",
        "        matrix=np.reshape(matrix,(5,5))\n",
        "\n",
        "        DirectionalMatrix=[]\n",
        "        for i in range(5):\n",
        "            row=[]\n",
        "            for j in range(5):\n",
        "                if(matrix[i][j]==0):\n",
        "                    row.append('\\u2190')    #left symbol\n",
        "                elif(matrix[i][j]==1):\n",
        "                    row.append('\\u2193')    #down symbol\n",
        "                elif(matrix[i][j]==2):\n",
        "                    row.append('\\u2192')    #right symbol\n",
        "                elif(matrix[i][j]==3):\n",
        "                    row.append('\\u2191')    #up symbol\n",
        "            DirectionalMatrix.append(row)\n",
        "\n",
        "        self.DirectionalMatrix=DirectionalMatrix\n",
        "        self.matrix=matrix\n",
        "        return matrix\n",
        "\n",
        "    # generate trajectories based on optimal actions\n",
        "    def getTrajectories(self,matrix,numTrajectories):\n",
        "        Trajectories=[]\n",
        "\n",
        "        for iters in range(numTrajectories):\n",
        "            path=[]       # list for a single trajectory\n",
        "            done=False\n",
        "            state = self.env.reset()\n",
        "            TotalReward = 0\n",
        "            path.append(state)\n",
        "            i=int(state/self.env.size)    # row index\n",
        "            j=state%self.env.size         # col index\n",
        "\n",
        "            # trajectory loop\n",
        "            while done != True:\n",
        "                action=matrix[i][j]       # retrieve action\n",
        "                i,j,state2, reward, done = self.env.step(action)    # take action\n",
        "                TotalReward += reward\n",
        "                state = state2          # update state\n",
        "                path.append(state)\n",
        "\n",
        "            Trajectories.append(path)\n",
        "\n",
        "        self.Trajectories=Trajectories\n",
        "        return Trajectories\n",
        "\n",
        "    # all training functions\n",
        "    def allInOne(self,model,numTrajectories):\n",
        "        self.env=model\n",
        "        Q=self.trainModel(model)\n",
        "        matrix=self.getDirections(Q)\n",
        "        return self.getTrajectories(matrix,numTrajectories)"
      ],
      "metadata": {
        "id": "cKSOh0qm9GyK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyMpFMqjNbDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}